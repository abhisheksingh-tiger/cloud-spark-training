{"cells":[{"cell_type":"markdown","source":["##Spark implements a subset of ANSI SQL:2003.\nWith the release of Spark 2.0, its authors created a superset of Hiveâ€™s support, writing a native\nSQL parser that supports both ANSI-SQL as well as HiveQL queries"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6c7b617a-e322-483c-91e3-93437ab7a1a4","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Spark provides a Java Database Connectivity (JDBC) interface by which either you or a remote\nprogram connects to the Spark driver in order to execute Spark SQL queries. A common use case\nmight be a for a business analyst to connect business intelligence software like Tableau to Spark.\nThe Thrift JDBC/Open Database Connectivity (ODBC) server implemented here corresponds to\nthe HiveServer2 in Hive 1.2.1. You can test the JDBC server with the beeline script that comes\nwith either Spark or Hive 1.2.1.\nTo start the JDBC/ODBC server, run the following in the Spark directory:\n./sbin/start-thriftserver.sh\nThis script accepts all bin/spark-submit command-line options. To see all available options\nfor configuring this Thrift Server, run ./sbin/start-thriftserver.sh --help. By default,\nthe server listens on localhost:10000. You can override this through environmental variables or\nsystem properties."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1f53460c-3864-42d9-84ec-d8082fc632e3","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["The core difference between tables and DataFrames is this: you define\nDataFrames in the scope of a programming language, whereas you define tables within a\ndatabase. This means that when you create a table (assuming you never changed the database), it\nwill belong to the default database.\n\nAn important thing to note is that in Spark 2.X, tables always contain data. There is no notion of\na temporary table, only a view, which does not contain data. This is important because if you go\nto drop a table, you can risk losing the data when doing so."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"95169ed2-e6ea-4282-a745-a05eada49ca8","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["The data within the tables as well as the data about the tables;\nthat is, the metadata. You can have Spark manage the metadata for a set of files as well as for the\ndata.\n\nWhen you define a table from files on disk, you are defining an unmanaged table. When\nyou use saveAsTable on a DataFrame, you are creating a managed table for which Spark will\ntrack of all of the relevant information.\n\nThis will read your table and write it out to a new location in Spark format. You can see this\nreflected in the new explain plan. In the explain plan, you will also notice that this writes to the\ndefault Hive warehouse location. You can set this by setting the spark.sql.warehouse.dir\nconfiguration to the directory of your choosing when you create your SparkSession. By default\nSpark sets this to /user/hive/warehouse:\n\nNote in the results that a database is listed. Spark also has databases which we will discuss later\nin this chapter, but for now you should keep in mind that you can also see tables in a specific\ndatabase by using the query show tables IN databaseName, where databaseName represents\nthe name of the database that you want to query.\nIf you are running on a new cluster or local mode, this should return zero results."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ef139cef-7dc8-46d3-b69b-02430f32ee5f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["table = spark.sql('''CREATE TABLE flights (\nDEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count LONG)\nUSING JSON OPTIONS (path 'dbfs:/FileStore/flight_data/2015_summary.json')''')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"760bb024-1efe-4749-9c57-d09cc0954df9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["type(table)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ed438e1e-2f1b-4ea7-9f76-84a1b9444700","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"Out[3]: pyspark.sql.dataframe.DataFrame","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[3]: pyspark.sql.dataframe.DataFrame"]}}],"execution_count":0},{"cell_type":"code","source":["table.show(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9ef59d12-e2a8-4743-8348-4d130cc3d94e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"++\n||\n++\n++\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["++\n||\n++\n++\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"Select * from flights limit 2\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cba13856-6436-495d-9357-87d1fe03a87e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|   15|\n|    United States|            Croatia|    1|\n+-----------------+-------------------+-----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|   15|\n|    United States|            Croatia|    1|\n+-----------------+-------------------+-----+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["spark.sql('''CREATE TABLE flights_dummy\nUSING JSON OPTIONS (path 'dbfs:/FileStore/flight_data/2015_summary.json')''')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"040a8bca-f49f-427a-8e23-8ad2671b9962","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"Out[9]: DataFrame[]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[9]: DataFrame[]"]}}],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"Select * from flights_dummy limit 2\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2951e38f-7383-4e2e-b2a0-dcb504798ae1","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|   15|\n|    United States|            Croatia|    1|\n+-----------------+-------------------+-----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|   15|\n|    United States|            Croatia|    1|\n+-----------------+-------------------+-----+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["table = spark.sql('''CREATE TABLE flights_csv (\nDEST_COUNTRY_NAME STRING, \nORIGIN_COUNTRY_NAME STRING COMMENT \"remember, the US will be most prevalent\", \ncount LONG)\nUSING JSON OPTIONS (path 'dbfs:/FileStore/flight_data/2015_summary.json')''')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a6bfc211-1ddc-4c20-9c4d-494d5fec0dde","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n\u001B[0;32m<command-61910737769275>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m table = spark.sql('''CREATE TABLE flights_csv (\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mDEST_COUNTRY_NAME\u001B[0m \u001B[0mSTRING\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mORIGIN_COUNTRY_NAME\u001B[0m \u001B[0mSTRING\u001B[0m \u001B[0mCOMMENT\u001B[0m \u001B[0;34m\"remember, the US will be most prevalent\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m count LONG)\n\u001B[1;32m      5\u001B[0m USING JSON OPTIONS (path 'dbfs:/FileStore/flight_data/2015_summary.json')''')\n\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/session.py\u001B[0m in \u001B[0;36msql\u001B[0;34m(self, sqlQuery, **kwargs)\u001B[0m\n\u001B[1;32m   1117\u001B[0m             \u001B[0msqlQuery\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mformatter\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msqlQuery\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1118\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1119\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsparkSession\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msqlQuery\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1120\u001B[0m         \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1121\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAnalysisException\u001B[0m: Table spark_catalog.default.flights_csv already exists.","errorSummary":"<span class='ansi-red-fg'>AnalysisException</span>: Table spark_catalog.default.flights_csv already exists.","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n\u001B[0;32m<command-61910737769275>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m table = spark.sql('''CREATE TABLE flights_csv (\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mDEST_COUNTRY_NAME\u001B[0m \u001B[0mSTRING\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mORIGIN_COUNTRY_NAME\u001B[0m \u001B[0mSTRING\u001B[0m \u001B[0mCOMMENT\u001B[0m \u001B[0;34m\"remember, the US will be most prevalent\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m count LONG)\n\u001B[1;32m      5\u001B[0m USING JSON OPTIONS (path 'dbfs:/FileStore/flight_data/2015_summary.json')''')\n\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/session.py\u001B[0m in \u001B[0;36msql\u001B[0;34m(self, sqlQuery, **kwargs)\u001B[0m\n\u001B[1;32m   1117\u001B[0m             \u001B[0msqlQuery\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mformatter\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msqlQuery\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1118\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1119\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsparkSession\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msqlQuery\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1120\u001B[0m         \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1121\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAnalysisException\u001B[0m: Table spark_catalog.default.flights_csv already exists."]}}],"execution_count":0},{"cell_type":"code","source":["#Describe table metadata\nspark.sql(\"DESCRIBE TABLE flights_csv\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7f1a113c-29ee-4df9-a519-7b8fe72ae61a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+-------------------+---------+--------------------+\n|           col_name|data_type|             comment|\n+-------------------+---------+--------------------+\n|  DEST_COUNTRY_NAME|   string|                null|\n|ORIGIN_COUNTRY_NAME|   string|remember, the US ...|\n|              count|   bigint|                null|\n+-------------------+---------+--------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------------+---------+--------------------+\n|           col_name|data_type|             comment|\n+-------------------+---------+--------------------+\n|  DEST_COUNTRY_NAME|   string|                null|\n|ORIGIN_COUNTRY_NAME|   string|remember, the US ...|\n|              count|   bigint|                null|\n+-------------------+---------+--------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"Select * from flights_csv limit 2\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"755a00a8-2af6-4fcf-8193-e13dce597b9f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|   15|\n|    United States|            Croatia|    1|\n+-----------------+-------------------+-----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|   15|\n|    United States|            Croatia|    1|\n+-----------------+-------------------+-----+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["#It is possible to create a table from a query as well:\nspark.sql('''\nCREATE TABLE flights_from_select USING parquet AS SELECT * FROM flights\n''')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"25912858-1c91-4807-bd8a-e426a8a50579","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"Out[17]: DataFrame[]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[17]: DataFrame[]"]}}],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"Select * from flights_from_select limit 2\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fde616ef-1504-4681-a4f5-1167e470cd93","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|   15|\n|    United States|            Croatia|    1|\n+-----------------+-------------------+-----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|   15|\n|    United States|            Croatia|    1|\n+-----------------+-------------------+-----+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# DROP TABLE IF EXISTS\nspark.sql('''\nDrop TABLE flights_from_select\n''')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"be7f61ac-b733-474f-9772-f3aead714ece","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"Out[16]: DataFrame[]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[16]: DataFrame[]"]}}],"execution_count":0},{"cell_type":"markdown","source":["Dropping unmanaged tables\nIf you are dropping an unmanaged table (e.g., hive_flights), no data will be removed but you\nwill no longer be able to refer to this data by the table name."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ed8a534d-ec7f-43a3-9e19-afd30dd34334","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#you can control the layout of the data by writing out a partitioned dataset,\nspark.sql('''\nCREATE TABLE partitioned_flights USING parquet PARTITIONED BY (DEST_COUNTRY_NAME)\nAS SELECT DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count FROM flights \n''')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7268119a-ba1b-4abf-b3a0-15611fbd89f9","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"Out[19]: DataFrame[]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[19]: DataFrame[]"]}}],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"SHOW PARTITIONS partitioned_flights\").show(20,False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"edbc997c-70ad-4856-981a-57e205b5cf03","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+---------------------------------------------------+\n|partition                                          |\n+---------------------------------------------------+\n|DEST_COUNTRY_NAME=Algeria                          |\n|DEST_COUNTRY_NAME=Angola                           |\n|DEST_COUNTRY_NAME=Anguilla                         |\n|DEST_COUNTRY_NAME=Antigua and Barbuda              |\n|DEST_COUNTRY_NAME=Argentina                        |\n|DEST_COUNTRY_NAME=Aruba                            |\n|DEST_COUNTRY_NAME=Australia                        |\n|DEST_COUNTRY_NAME=Austria                          |\n|DEST_COUNTRY_NAME=Azerbaijan                       |\n|DEST_COUNTRY_NAME=Bahrain                          |\n|DEST_COUNTRY_NAME=Barbados                         |\n|DEST_COUNTRY_NAME=Belgium                          |\n|DEST_COUNTRY_NAME=Belize                           |\n|DEST_COUNTRY_NAME=Bermuda                          |\n|DEST_COUNTRY_NAME=Bolivia                          |\n|DEST_COUNTRY_NAME=Bonaire, Sint Eustatius, and Saba|\n|DEST_COUNTRY_NAME=Brazil                           |\n|DEST_COUNTRY_NAME=British Virgin Islands           |\n|DEST_COUNTRY_NAME=Bulgaria                         |\n|DEST_COUNTRY_NAME=Burkina Faso                     |\n+---------------------------------------------------+\nonly showing top 20 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---------------------------------------------------+\n|partition                                          |\n+---------------------------------------------------+\n|DEST_COUNTRY_NAME=Algeria                          |\n|DEST_COUNTRY_NAME=Angola                           |\n|DEST_COUNTRY_NAME=Anguilla                         |\n|DEST_COUNTRY_NAME=Antigua and Barbuda              |\n|DEST_COUNTRY_NAME=Argentina                        |\n|DEST_COUNTRY_NAME=Aruba                            |\n|DEST_COUNTRY_NAME=Australia                        |\n|DEST_COUNTRY_NAME=Austria                          |\n|DEST_COUNTRY_NAME=Azerbaijan                       |\n|DEST_COUNTRY_NAME=Bahrain                          |\n|DEST_COUNTRY_NAME=Barbados                         |\n|DEST_COUNTRY_NAME=Belgium                          |\n|DEST_COUNTRY_NAME=Belize                           |\n|DEST_COUNTRY_NAME=Bermuda                          |\n|DEST_COUNTRY_NAME=Bolivia                          |\n|DEST_COUNTRY_NAME=Bonaire, Sint Eustatius, and Saba|\n|DEST_COUNTRY_NAME=Brazil                           |\n|DEST_COUNTRY_NAME=British Virgin Islands           |\n|DEST_COUNTRY_NAME=Bulgaria                         |\n|DEST_COUNTRY_NAME=Burkina Faso                     |\n+---------------------------------------------------+\nonly showing top 20 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["These tables will be available in Spark even through sessions; temporary tables do not currently\nexist in Spark. You must create a temporary view, which we demonstrate later in this chapter.\n\nInsertion:\n\nINSERT INTO flights_from_select\nSELECT DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count FROM flights LIMIT 20\nYou can optionally provide a partition specification if you want to write only into a certain\npartition. Note that a write will respect a partitioning scheme, as well (which may cause the\nabove query to run quite slowly); however, it will add additional files only into the end\npartitions:\nINSERT INTO partitioned_flights\nPARTITION (DEST_COUNTRY_NAME=\"UNITED STATES\")\nSELECT count, ORIGIN_COUNTRY_NAME FROM flights\nWHERE DEST_COUNTRY_NAME='UNITED STATES' LIMIT 12"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b9d50f68-0ddd-4a61-b9a0-205b854061f3","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["REFRESH TABLE refreshes\nall cached entries (essentially, files) associated with the table. If the table were previously\ncached, it would be cached lazily the next time it is scanned:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9d82d58f-61f8-490f-8bd4-07674465e084","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["spark.sql(\"REFRESH table flights_csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"372351b8-81ee-40cc-82ce-2b9e31026a2e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"Out[7]: DataFrame[]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[7]: DataFrame[]"]}}],"execution_count":0},{"cell_type":"markdown","source":["Another related command is REPAIR TABLE, which refreshes the partitions maintained in the\ncatalog for that given table. This commandâ€™s focus is on collecting new partition informationâ€”\nan example might be writing out a new partition manually and the need to repair the table\naccordingly:\nMSCK REPAIR TABLE partitioned_flights"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e397e333-330f-43a7-87a4-95e58f1321fb","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Just like DataFrames, you can cache and uncache tables. You simply specify which table you\nwould like using the following syntax:\n\nCACHE TABLE flights\n\nHereâ€™s how you uncache them:\n\nUNCACHE TABLE FLIGHTS"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b7b7b639-f3e0-4fca-96c8-f4e2be359656","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["view specifies a set\nof transformations on top of an existing tableâ€”basically just saved query plans, which can be\nconvenient for organizing or reusing your query logic. Views can be global, set to a database, or per session.\n\nTo an end user, views are displayed as tables, except rather than rewriting all of the data to a new\nlocation, they simply perform a transformation on the source data at query time. This might be a\nfilter, select, or potentially an even larger GROUP BY or ROLLUP. For instance, in the\nfollowing example, we create a view in which the destination is United States in order to see\nonly those flights"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"edb697fb-d4d1-49c2-9ace-6a274a12e4b8","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["spark.sql('''\nCREATE VIEW just_usa_view AS\nSELECT * FROM flights WHERE dest_country_name = 'United States'\n''')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"de3a10b2-912d-4512-be41-b15feff634fc","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"Out[8]: DataFrame[]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[8]: DataFrame[]"]}}],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"Select * from just_usa_view limit 5\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"930071a8-8049-4cfe-8036-704c3be9002c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|   15|\n|    United States|            Croatia|    1|\n|    United States|            Ireland|  344|\n|    United States|              India|   62|\n|    United States|          Singapore|    1|\n+-----------------+-------------------+-----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|   15|\n|    United States|            Croatia|    1|\n|    United States|            Ireland|  344|\n|    United States|              India|   62|\n|    United States|          Singapore|    1|\n+-----------------+-------------------+-----+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["Like tables, you can create temporary views that are available only during the current session and\nare not registered to a database:\n\nCREATE TEMP VIEW just_usa_view_temp AS\nSELECT * FROM flights WHERE dest_country_name = 'United States'\n\nOr, it can be a global temp view. Global temp views are resolved regardless of database and are\nviewable across the entire Spark application, but they are removed at the end of the session:\n\nCREATE GLOBAL TEMP VIEW just_usa_global_view_temp AS\nSELECT * FROM flights WHERE dest_country_name = 'United States'"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fa007fbf-ba88-43ad-bec5-05836f1d7be6","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["You can also specify that you would like to overwite a view if one already exists by using the\nkeywords shown in the sample that follows. We can overwrite both temp views and regular\nviews:\n\nCREATE OR REPLACE TEMP VIEW just_usa_view_temp AS\n\nSELECT * FROM flights WHERE dest_country_name = 'United States'\n\nEffectively, views are equivalent to creating a new DataFrame from an existing DataFrame"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"86859d48-0fd5-47a4-a757-b0c417697933","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["spark.sql(\"DROP VIEW IF EXISTS just_usa_view\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6d0a45d7-fc20-45de-b1be-506176d16830","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"Out[10]: DataFrame[]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[10]: DataFrame[]"]}}],"execution_count":0},{"cell_type":"markdown","source":["Databases are a tool for organizing tables. As mentioned earlier, if you do not define one, Spark\nwill use the default database. Any SQL statements that you run from within Spark (including\nDataFrame commands) execute within the context of a database. This means that if you change\nthe database, any user-defined tables will remain in the previous database and will need to be\nqueried differently.\n\nThis can be a source of confusion, especially if youâ€™re sharing the same context or session for your\ncoworkers, so be sure to set your databases appropriately"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"00076886-70f9-4baa-b9e6-bf9c33ba2d2b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["spark.sql(\"SHOW DATABASES\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"85274ef0-cea6-42dc-92f4-cc1cafa86004","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+------------+\n|databaseName|\n+------------+\n|     default|\n+------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+------------+\n|databaseName|\n+------------+\n|     default|\n+------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"CREATE DATABASE demo_db\")\n#to use a specific db\nspark.sql(\"USE demo_db\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"76da8928-955b-45ef-9468-8764c153aba4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"Out[14]: DataFrame[]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[14]: DataFrame[]"]}}],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"Show tables\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f70321ac-466a-43db-8ee7-9c7d5dd22282","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+--------+---------+-----------+\n|database|tableName|isTemporary|\n+--------+---------+-----------+\n+--------+---------+-----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------+---------+-----------+\n|database|tableName|isTemporary|\n+--------+---------+-----------+\n+--------+---------+-----------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["#spark.sql(\"SELECT * FROM flights limit 5\").show() # should fail\nspark.sql(\"SELECT * FROM default.flights limit 5\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7ef74604-6f66-4a46-8860-925045394d31","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|   15|\n|    United States|            Croatia|    1|\n|    United States|            Ireland|  344|\n|            Egypt|      United States|   15|\n|    United States|              India|   62|\n+-----------------+-------------------+-----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|   15|\n|    United States|            Croatia|    1|\n|    United States|            Ireland|  344|\n|            Egypt|      United States|   15|\n|    United States|              India|   62|\n+-----------------+-------------------+-----+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["You can see what database youâ€™re currently using by running the following command:\n\nSELECT current_database()\n\nYou can, of course, switch back to the default database:\n\nUSE default;"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"51df74ea-3216-40ec-9642-8293eeef84fc","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["spark.sql(\"DROP DATABASE IF EXISTS demo_db;\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"10fd24b3-d815-43a9-9ebf-bc6315545e2a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"Out[19]: DataFrame[]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[19]: DataFrame[]"]}}],"execution_count":0},{"cell_type":"markdown","source":["Queries in Spark support the following ANSI SQL requirements"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0f6ff991-f1d4-4697-a77d-37afc396c912","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["spark.sql(\"SELECT current_database()\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8993c25c-6401-4d91-92d2-25b4c5727301","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+------------------+\n|current_database()|\n+------------------+\n|           demo_db|\n+------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+------------------+\n|current_database()|\n+------------------+\n|           demo_db|\n+------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"Use default\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"40f5f54e-7eb5-4158-be4d-da7f89199284","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"Out[23]: DataFrame[]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[23]: DataFrame[]"]}}],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"DROP DATABASE IF EXISTS demo_db;\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9ea8bc35-4561-4ebe-9561-a3cbe72e6982","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"Out[24]: DataFrame[]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[24]: DataFrame[]"]}}],"execution_count":0},{"cell_type":"code","source":["#if statements:\nspark.sql('''\nSELECT\n*, (CASE WHEN DEST_COUNTRY_NAME = 'United States' THEN 1\nWHEN DEST_COUNTRY_NAME = 'Egypt' THEN 0\nELSE -1 END) as condition\nFROM flights\n''').show(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3e5dcf72-1750-4f51-8215-7663b451bc2d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+-----------------+-------------------+-----+---------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|condition|\n+-----------------+-------------------+-----+---------+\n|    United States|            Romania|   15|        1|\n|    United States|            Croatia|    1|        1|\n|    United States|            Ireland|  344|        1|\n|            Egypt|      United States|   15|        0|\n|    United States|              India|   62|        1|\n|    United States|          Singapore|    1|        1|\n|    United States|            Grenada|   62|        1|\n|       Costa Rica|      United States|  588|       -1|\n|          Senegal|      United States|   40|       -1|\n|          Moldova|      United States|    1|       -1|\n+-----------------+-------------------+-----+---------+\nonly showing top 10 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------------+-------------------+-----+---------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|condition|\n+-----------------+-------------------+-----+---------+\n|    United States|            Romania|   15|        1|\n|    United States|            Croatia|    1|        1|\n|    United States|            Ireland|  344|        1|\n|            Egypt|      United States|   15|        0|\n|    United States|              India|   62|        1|\n|    United States|          Singapore|    1|        1|\n|    United States|            Grenada|   62|        1|\n|       Costa Rica|      United States|  588|       -1|\n|          Senegal|      United States|   40|       -1|\n|          Moldova|      United States|    1|       -1|\n+-----------------+-------------------+-----+---------+\nonly showing top 10 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["Complex types are a departure from standard SQL and are an incredibly powerful feature that\ndoes not exist in standard SQL. Understanding how to manipulate them appropriately in SQL is\nessential. There are three core complex types in Spark SQL: structs, lists, and maps.\n\nStructs are more akin to maps. They provide a way of creating or querying nested data in Spark.\nTo create one, you simply need to wrap a set of columns (or expressions) in parentheses:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"68ba4c3f-a448-437f-b757-0b7eb392d36a","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["spark.sql('''CREATE VIEW IF NOT EXISTS nested_data AS\nSELECT (DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME) as country, count FROM flights''')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"866cef54-0e65-45ee-b167-e9fdcc9ead94","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"Out[30]: DataFrame[]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[30]: DataFrame[]"]}}],"execution_count":0},{"cell_type":"code","source":["spark.sql(''' select * from nested_data''').show(5,False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"54b0caea-4728-4cc9-8857-0f7d285f7fd0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+------------------------+-----+\n|country                 |count|\n+------------------------+-----+\n|{United States, Romania}|15   |\n|{United States, Croatia}|1    |\n|{United States, Ireland}|344  |\n|{Egypt, United States}  |15   |\n|{United States, India}  |62   |\n+------------------------+-----+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+------------------------+-----+\n|country                 |count|\n+------------------------+-----+\n|{United States, Romania}|15   |\n|{United States, Croatia}|1    |\n|{United States, Ireland}|344  |\n|{Egypt, United States}  |15   |\n|{United States, India}  |62   |\n+------------------------+-----+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["spark.sql(''' select country.DEST_COUNTRY_NAME, count from nested_data''').show(5,False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"80b31558-288c-47ae-8cbc-e2d84274bd60","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+-----------------+-----+\n|DEST_COUNTRY_NAME|count|\n+-----------------+-----+\n|United States    |15   |\n|United States    |1    |\n|United States    |344  |\n|Egypt            |15   |\n|United States    |62   |\n+-----------------+-----+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------------+-----+\n|DEST_COUNTRY_NAME|count|\n+-----------------+-----+\n|United States    |15   |\n|United States    |1    |\n|United States    |344  |\n|Egypt            |15   |\n|United States    |62   |\n+-----------------+-----+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["You can use the collect_list function,\nwhich creates a list of values. You can also use the function collect_set, which creates an\narray without duplicate values. These are both aggregation functions"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f742adf8-d0e8-44f5-80cb-d9307927be0b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["spark.sql('''\nSELECT DEST_COUNTRY_NAME as new_name, collect_list(count) as flight_counts,\ncollect_set(ORIGIN_COUNTRY_NAME) as origin_set\nFROM flights GROUP BY DEST_COUNTRY_NAME\n''').show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6a2ab1fa-5f05-4455-85e5-93e86a9e94ba","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+-------------------+-------------+---------------+\n|           new_name|flight_counts|     origin_set|\n+-------------------+-------------+---------------+\n|            Algeria|          [4]|[United States]|\n|             Angola|         [15]|[United States]|\n|           Anguilla|         [41]|[United States]|\n|Antigua and Barbuda|        [126]|[United States]|\n|          Argentina|        [180]|[United States]|\n+-------------------+-------------+---------------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------------+-------------+---------------+\n|           new_name|flight_counts|     origin_set|\n+-------------------+-------------+---------------+\n|            Algeria|          [4]|[United States]|\n|             Angola|         [15]|[United States]|\n|           Anguilla|         [41]|[United States]|\n|Antigua and Barbuda|        [126]|[United States]|\n|          Argentina|        [180]|[United States]|\n+-------------------+-------------+---------------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["You can, however, also create an array manually within a column, as shown here:\nSELECT DEST_COUNTRY_NAME, ARRAY(1, 2, 3) FROM flights"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d473dcac-ce45-4ede-a1a0-2d1ec01e8c19","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["You can also query lists by position by using a Python-like array query syntax:\n\nSELECT DEST_COUNTRY_NAME as new_name, collect_list(count)[0]\n\nFROM flights GROUP BY DEST_COUNTRY_NAME"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"27712f68-534c-41b2-b07a-20be2d150206","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["You can also do things like convert an array back into rows. You do this by using the explode\nfunction."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"582e2477-de13-416a-ab37-77c82313569d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["spark.sql('''\nCREATE OR REPLACE TEMP VIEW flights_agg AS\nSELECT DEST_COUNTRY_NAME, collect_list(count) as collected_counts\nFROM flights GROUP BY DEST_COUNTRY_NAME\n''')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"12477ebf-26af-45f0-adc4-93289b8149e3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"Out[42]: DataFrame[]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[42]: DataFrame[]"]}}],"execution_count":0},{"cell_type":"code","source":["spark.sql('''\nSELECT explode(collected_counts), DEST_COUNTRY_NAME FROM flights_agg\n''').show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1f1ce498-c7a9-4a33-8e61-4855b2046bee","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+---+-------------------+\n|col|  DEST_COUNTRY_NAME|\n+---+-------------------+\n|  4|            Algeria|\n| 15|             Angola|\n| 41|           Anguilla|\n|126|Antigua and Barbuda|\n|180|          Argentina|\n+---+-------------------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+-------------------+\n|col|  DEST_COUNTRY_NAME|\n+---+-------------------+\n|  4|            Algeria|\n| 15|             Angola|\n| 41|           Anguilla|\n|126|Antigua and Barbuda|\n|180|          Argentina|\n+---+-------------------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"SHOW FUNCTIONS\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"28a415c1-b6ec-414c-a684-fc3480aaf87d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+--------+\n|function|\n+--------+\n|       !|\n|      !=|\n|       %|\n|       &|\n|       *|\n|       +|\n|       -|\n|       /|\n|       <|\n|      <=|\n|     <=>|\n|      <>|\n|       =|\n|      ==|\n|       >|\n|      >=|\n|       ^|\n|     abs|\n|    acos|\n|   acosh|\n+--------+\nonly showing top 20 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------+\n|function|\n+--------+\n|       !|\n|      !=|\n|       %|\n|       &|\n|       *|\n|       +|\n|       -|\n|       /|\n|       <|\n|      <=|\n|     <=>|\n|      <>|\n|       =|\n|      ==|\n|       >|\n|      >=|\n|       ^|\n|     abs|\n|    acos|\n|   acosh|\n+--------+\nonly showing top 20 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"SHOW SYSTEM FUNCTIONS\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4f23e208-7b4d-4625-85dd-d4c85d4c5145","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+--------+\n|function|\n+--------+\n|       !|\n|      !=|\n|       %|\n|       &|\n|       *|\n|       +|\n|       -|\n|       /|\n|       <|\n|      <=|\n|     <=>|\n|      <>|\n|       =|\n|      ==|\n|       >|\n|      >=|\n|       ^|\n|     abs|\n|    acos|\n|   acosh|\n+--------+\nonly showing top 20 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------+\n|function|\n+--------+\n|       !|\n|      !=|\n|       %|\n|       &|\n|       *|\n|       +|\n|       -|\n|       /|\n|       <|\n|      <=|\n|     <=>|\n|      <>|\n|       =|\n|      ==|\n|       >|\n|      >=|\n|       ^|\n|     abs|\n|    acos|\n|   acosh|\n+--------+\nonly showing top 20 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"SHOW USER FUNCTIONS\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"398c6b5a-8fae-4baf-a4bf-2c903ae7a973","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+--------+\n|function|\n+--------+\n+--------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------+\n|function|\n+--------+\n+--------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"SHOW FUNCTIONS 's*'\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b756595c-97da-4a20-8370-ec5a65f498e8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+------------------+\n|          function|\n+------------------+\n|     schema_of_csv|\n|    schema_of_json|\n|               sec|\n|            second|\n|            secret|\n|         sentences|\n|          sequence|\n|    session_window|\n|               sha|\n|              sha1|\n|              sha2|\n|         shiftleft|\n|        shiftright|\n|shiftrightunsigned|\n|           shuffle|\n|              sign|\n|            signum|\n|               sin|\n|              sinh|\n|              size|\n+------------------+\nonly showing top 20 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+------------------+\n|          function|\n+------------------+\n|     schema_of_csv|\n|    schema_of_json|\n|               sec|\n|            second|\n|            secret|\n|         sentences|\n|          sequence|\n|    session_window|\n|               sha|\n|              sha1|\n|              sha2|\n|         shiftleft|\n|        shiftright|\n|shiftrightunsigned|\n|           shuffle|\n|              sign|\n|            signum|\n|               sin|\n|              sinh|\n|              size|\n+------------------+\nonly showing top 20 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"DESCRIBE FUNCTION 'SEQUENCE'\").show(5,False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cd8b51d6-cdbb-4d21-a41f-198be145148a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|function_desc                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|Function: sequence                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n|Class: org.apache.spark.sql.catalyst.expressions.Sequence                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n|Usage: \\n    sequence(start, stop, step) - Generates an array of elements from start to stop (inclusive),\\n      incrementing by step. The type of the returned elements is the same as the type of argument\\n      expressions.\\n\\n      Supported types are: byte, short, integer, long, date, timestamp.\\n\\n      The start and stop expressions must resolve to the same type.\\n      If start and stop expressions resolve to the 'date' or 'timestamp' type\\n      then the step expression must resolve to the 'interval' or 'year-month interval' or\\n      'day-time interval' type, otherwise to the same type as the start and stop expressions.\\n  |\n+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|function_desc                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|Function: sequence                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n|Class: org.apache.spark.sql.catalyst.expressions.Sequence                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n|Usage: \\n    sequence(start, stop, step) - Generates an array of elements from start to stop (inclusive),\\n      incrementing by step. The type of the returned elements is the same as the type of argument\\n      expressions.\\n\\n      Supported types are: byte, short, integer, long, date, timestamp.\\n\\n      The start and stop expressions must resolve to the same type.\\n      If start and stop expressions resolve to the 'date' or 'timestamp' type\\n      then the step expression must resolve to the 'interval' or 'year-month interval' or\\n      'day-time interval' type, otherwise to the same type as the start and stop expressions.\\n  |\n+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["\npower3 = lambda number : number * number * number\nspark.udf.register(\"power3\", power3)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8c88db4e-47ce-435a-a35d-29503e6e7cca","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"Out[15]: <function __main__.<lambda>(number)>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[15]: <function __main__.<lambda>(number)>"]}}],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"SHOW USER FUNCTIONS\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5362aea3-b0ae-4f84-bc50-afa95b41f4cd","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+--------+\n|function|\n+--------+\n|  power3|\n+--------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------+\n|function|\n+--------+\n|  power3|\n+--------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"SELECT count, power3(count) FROM flights\").show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3a76003c-ede0-4d00-993e-2fbf2dfbed3e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+-----+-------------+\n|count|power3(count)|\n+-----+-------------+\n|   15|         3375|\n|    1|            1|\n|  344|     40707584|\n|   15|         3375|\n|   62|       238328|\n+-----+-------------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----+-------------+\n|count|power3(count)|\n+-----+-------------+\n|   15|         3375|\n|    1|            1|\n|  344|     40707584|\n|   15|         3375|\n|   62|       238328|\n+-----+-------------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["Correlated subqueries use some information from the outer scope of the query in\norder to supplement information in the subquery. Uncorrelated subqueries include no\ninformation from the outer scope. Each of these queries can return one (scalar subquery) or more\nvalues. Spark also includes support for predicate subqueries, which allow for filtering based on\nvalues."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"46cafd62-a3d3-4a4a-a2c7-aae60e11d1aa","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["spark.sql('''\nSELECT dest_country_name FROM flights\nGROUP BY dest_country_name ORDER BY sum(count) DESC LIMIT 5\n''').show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"247997ff-d554-4bdf-bb81-633db5f6c550","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+-----------------+\n|dest_country_name|\n+-----------------+\n|    United States|\n|           Canada|\n|           Mexico|\n|   United Kingdom|\n|            Japan|\n+-----------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------------+\n|dest_country_name|\n+-----------------+\n|    United States|\n|           Canada|\n|           Mexico|\n|   United Kingdom|\n|            Japan|\n+-----------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Uncorrelated predicate subqueries\nspark.sql('''\nSELECT * FROM flights\nWHERE origin_country_name IN (SELECT dest_country_name FROM flights\nGROUP BY dest_country_name ORDER BY sum(count) DESC LIMIT 5)\n''').show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"79a01e9e-9e52-4c09-90df-630e0a22e603","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|            Egypt|      United States|   15|\n|       Costa Rica|      United States|  588|\n|          Senegal|      United States|   40|\n|          Moldova|      United States|    1|\n|           Guyana|      United States|   64|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|            Egypt|      United States|   15|\n|       Costa Rica|      United States|  588|\n|          Senegal|      United States|   40|\n|          Moldova|      United States|    1|\n|           Guyana|      United States|   64|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Correlated predicate subqueries\nspark.sql(\n'''\nSELECT * FROM flights f1\nWHERE EXISTS (SELECT 1 FROM flights f2\nWHERE f1.dest_country_name = f2.origin_country_name)\nAND EXISTS (SELECT 1 FROM flights f2\nWHERE f2.dest_country_name = f1.origin_country_name)\n''').show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5db02adb-6857-45e9-9b14-e7b1acb82af6","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|   15|\n|    United States|            Croatia|    1|\n|    United States|            Ireland|  344|\n|            Egypt|      United States|   15|\n|    United States|              India|   62|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|   15|\n|    United States|            Croatia|    1|\n|    United States|            Ireland|  344|\n|            Egypt|      United States|   15|\n|    United States|              India|   62|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Uncorrelated scalar queries\nspark.sql(\"SELECT *, (SELECT max(count) FROM flights) AS maximum FROM flights\").show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"658efc85-9592-4c44-af53-3a8fd73b6a82","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+-----------------+-------------------+-----+-------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|maximum|\n+-----------------+-------------------+-----+-------+\n|    United States|            Romania|   15| 370002|\n|    United States|            Croatia|    1| 370002|\n|    United States|            Ireland|  344| 370002|\n|            Egypt|      United States|   15| 370002|\n|    United States|              India|   62| 370002|\n+-----------------+-------------------+-----+-------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------------+-------------------+-----+-------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|maximum|\n+-----------------+-------------------+-----+-------+\n|    United States|            Romania|   15| 370002|\n|    United States|            Croatia|    1| 370002|\n|    United States|            Ireland|  344| 370002|\n|            Egypt|      United States|   15| 370002|\n|    United States|              India|   62| 370002|\n+-----------------+-------------------+-----+-------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"83635050-0f42-4d2b-9053-a5132331e945","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"practice7chap10","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1877296319107463}},"nbformat":4,"nbformat_minor":0}
