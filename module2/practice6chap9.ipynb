{"cells":[{"cell_type":"markdown","source":["Following are Spark’s core data\nsources:\n- CSV\n- JSON\n- Parquet\n- ORC\n- JDBC/ODBC connections\n- Plain-text files\nAs mentioned, Spark has numerous community-created data sources. Here’s just a small sample:\n- Cassandra\n- HBase\n- MongoDB\n- AWS Redshift\n- XML, etc.\n\nDataFrameReader.format(...).option(\"key\", \"value\").schema(...).load()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d8099033-9039-4a4c-85f2-470644259511","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["'''spark.read.format(\"csv\")\n.option(\"mode\", \"FAILFAST\")\n.option(\"inferSchema\", \"true\")\n.option(\"path\", \"path/to/file(s)\")\n.schema(someSchema)\n.load()'''"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2ab99411-c590-4c89-89e5-a87769891908","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":[".option(\"mode\", \"FAILFAST\")\n- permissive: Sets all fields to null when it encounters a corrupted record and places all corrupted records\nin a string column called _corrupt_record\n- dropMalformed Drops the row that contains malformed records\n- failFast Fails immediately upon encountering malformed records"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"804ae58f-291b-4f2a-9b0d-9eab5c9a4c3c","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["DataFrameWriter.format(...).option(...).partitionBy(...).bucketBy(...).sortBy(...).save()\n\nformat is optional because by default,\nSpark will use the parquet format. option, again, allows us to configure how to write out our\ngiven data. PartitionBy, bucketBy, and sortBy work only for file-based data sources; you can\nuse them to control the specific layout of files at the destination.\n\ndataframe.write.format(\"csv\")\n.option(\"mode\", \"OVERWRITE\")\n.option(\"dateFormat\", \"yyyy-MM-dd\")\n.option(\"path\", \"path/to/file(s)\")\n.save()\n\nSave mode Description\n- append Appends the output files to the list of files that already exist at that location\n- overwrite Will completely overwrite any data that already exists there\n- errorIfExists Throws an error and fails the write if data or files already exist at the specified location\n- ignore If data or files exist at the location, do nothing with the current DataFrame\n\nThe default is errorIfExists. This means that if Spark finds data at the location to which\nyou’re writing, it will fail the write immediately."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"33d9e2c6-d2a1-4391-a03a-bd94694e05d7","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Writing csv file as TSV file\n\ncsvFileDF.write.format(\"csv\").mode(\"overwrite\").option(\"sep\", \"\\t\")\\\n.save(\"/tmp/my-tsv-file.tsv\")\n\nWhen you list the destination directory, you can see that my-tsv-file is actually a folder with\nnumerous files within it, This actually reflects the number of partitions in our DataFrame at the time we write it out."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b8959cf9-aba9-42ce-a19d-da8055cec3a1","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["In Spark, when we refer to JSON files, we refer to\nline-delimited JSON files. This contrasts with files that have a large JSON object or array per\nfile.\nThe line-delimited versus multiline trade-off is controlled by a single option: **multiLine**. When\nyou set this option to true, you can read an entire file as one json object . Line-delimited JSON is actually a much more\nstable format because it allows you to append to a file with a new record (rather than having to\nread in an entire file and then write it out)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"762a5a38-3871-4a4b-80ad-27cd8d5bb525","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df_json=spark.read.format(\"json\").option(\"mode\", \"FAILFAST\")\\\n.option(\"inferSchema\", \"true\")\\\n.load(\"dbfs:/FileStore/flight_data/2010_summary.json\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fa9ed681-c64e-4d0e-8cc7-32356b7689f5","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df_json.write.format(\"json\").mode(\"overwrite\").save(\"dbfs:/FileStore/tmp/my-json-file.json\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3f4affde-c8b3-4276-a087-f1b220d4eab5","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Writing JSON files is just as simple as reading them, and, as you might expect, the data source\ndoes not matter. Therefore, we can reuse the CSV DataFrame to be the\nsource for our JSON file."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0e65d0b0-b6e6-4546-b926-76d7efc755ac","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["##Parquet Files\nParquet is an open source column-oriented data store that provides a variety of storage\noptimizations, especially for analytics workloads. It provides columnar compression, which\nsaves storage space and allows for reading individual columns instead of entire files.\n\nWe\nrecommend writing data out to Parquet for long-term storage because reading from a Parquet file\nwill always be more efficient than JSON or CSV. Another advantage of Parquet is that it\nsupports complex types. This means that if your column is an array (which would fail with a\nCSV file, for example), map, or struct, you’ll still be able to read and write that file without\nissue.\n\nParquet has very few options because it enforces its own schema when storing data. Thus, all you\nneed to set is the format and you are good to go.\n\n*Note: Even though there are only two options, you can still encounter problems if you’re working with\nincompatible Parquet files. Be careful when you write out Parquet files with different versions of\nSpark (especially older ones) because this can cause significant headache.*\n\n**Read Format:**\n\nspark.read.format(\"parquet\").load(\"/data/flight-data/parquet/2010-summary.parquet\").show(5)\n\n**Write Format**\n\ncsvFile.write.format(\"parquet\").mode(\"overwrite\").save(\"/tmp/my-parquet-file.parquet\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4b9e7da7-aee3-4258-93ea-d3a3b4e3000e","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["ORC Files\nORC is a self-describing, type-aware columnar file format designed for Hadoop workloads. It is\noptimized for large streaming reads, but with integrated support for finding required rows\nquickly.\n\nthe fundamental difference is that Parquet is further\noptimized for use with Spark, whereas ORC is further optimized for Hive\n\nspark.read.format(\"orc\").load(\"/data/flight-data/orc/2010-summary.orc\").show(5)\n\ncsvFile.write.format(\"orc\").mode(\"overwrite\").save(\"/tmp/my-json-file.orc\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b42bf147-9939-41aa-b9fb-7835dd982b63","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["SQL Databases\n\nTo read and write from these databases, you need to do two things: include the Java Database\nConnectivity (JDBC) driver for you particular database on the spark classpath, and provide the\nproper JAR for the driver itself\n\nFor example, to be able to read and write from PostgreSQL, you\nmight run something like this:\n\n./bin/spark-shell --driver-class-path postgresql-9.4.1207.jar --jars postgresql-9.4.1207.jar\n\nRead Format\n\ndriver = \"org.sqlite.JDBC\"\npath = \"/data/flight-data/jdbc/my-sqlite.db\"\nurl = \"jdbc:sqlite:\" + path\ntablename = \"flight_info\"\n\ndbDataFrame = spark.read.format(\"jdbc\").option(\"url\", url).option(\"dbtable\", tablename).option(\"driver\", driver).load()\n\npgDF = spark.read.format(\"jdbc\")\n.option(\"driver\", \"org.postgresql.Driver\")\n.option(\"url\", \"jdbc:postgresql://database_server\")\n.option(\"dbtable\", \"schema.tablename\")\n.option(\"user\", \"username\").option(\"password\", \"my-secret-password\").load()\n\nWrite Format\n\nnewPath = \"jdbc:sqlite://tmp/my-sqlite.db\"\ncsvFile.write.jdbc(newPath, tablename, mode=\"overwrite\", properties=props)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"26a323de-7411-4b8c-9947-325efc0faffd","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["- 9,10,15 Thu\n- 15,18,19 Fri\n- module 1 link2, module 2 task 2 Sat\n- capstone Sun"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f1d92343-7eae-435c-8122-c985ee5cbd35","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["As an example of how you would do\nthis, suppose that you need to parse some Apache log files to some more structured format, or\nperhaps you want to parse some plain text for natural-language processing.\n\nRead Format\n\nspark.read.textFile(\"/data/flight-data/csv/2010-summary.csv\")\n.selectExpr(\"split(value, ',') as rows\").show()\n\nWriting Text Files\n\nWhen you write a text file, you need to be sure to have only one string column; otherwise, the\nwrite will fail:\n\ncsvFile.select(\"DEST_COUNTRY_NAME\").write.text(\"/tmp/simple-text-file.txt\")\n\nIf you perform some partitioning when performing your write, you can write more columns. However, those columns will manifest as\ndirectories in the folder to which you’re writing out to, instead of columns on every single file\n\ncsvFile.limit(10).select(\"DEST_COUNTRY_NAME\", \"count\")\n.write.partitionBy(\"count\").text(\"/tmp/five-csv-files2py.csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"966ffdfa-e30b-4d5f-9929-54d17b60df74","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["##Advanced I/O Concepts\nWe saw previously that we can control the parallelism of files that we write by controlling the\npartitions prior to writing. We can also control specific data layout by controlling two things:\nbucketing and partitioning\n\nCertain file formats are fundamentally “splittable.” This can improve speed because it makes it\npossible for Spark to avoid reading an entire file, and access only the parts of the file necessary\nto satisfy your query.\nIn conjunction with this is a need to manage compression. Not all compression schemes\nare splittable. How you store your data is of immense consequence when it comes to making\nyour Spark jobs run smoothly. We recommend Parquet with gzip compression."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3d5aea88-a0ad-49ce-a0ad-79f741e5c0ee","inputWidgets":{},"title":""}}},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a999b953-7116-4c61-9f28-009c00a62209","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"practice6chap9","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1364550289959339}},"nbformat":4,"nbformat_minor":0}
